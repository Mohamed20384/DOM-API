import os
import io
import json
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from fastapi import FastAPI, Request, Form, UploadFile, File
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.templating import Jinja2Templates
from dotenv import load_dotenv
import PyPDF2
import google.generativeai as genai
from langchain.text_splitter import CharacterTextSplitter

from langchain_core.runnables import RunnableMap, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain.retrievers import ContextualCompressionRetriever
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware

# Import our processing function
from app.processing.json_to_pdf import process_restaurants_json

# Load environment variables
load_dotenv()
GOOGLE_API_KEY = os.getenv("GENAI_API_KEY")
genai.configure(api_key=GOOGLE_API_KEY)

app = FastAPI(title="DOM - Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ù…ØµØ±ÙŠ")
templates = Jinja2Templates(directory="app/templates")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For development only! Restrict in production.
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration (same as before)
class Config:
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 300
    EMBEDDING_BATCH_SIZE = 10
    MAX_PREVIEW_CHARS = 5000
    PDF_FOLDER = "Restaurants_PDF"
    SYSTEM_PROMPT = """
    Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ø§Ø³Ù…Ùƒ DOMØŒ Ø´ØºØ§Ù„ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ù…Ø·Ø§Ø¹Ù… Ø¯Ù…ÙŠØ§Ø·. Ø¨ØªØªÙƒÙ„Ù… Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©ØŒ ÙˆÙ…Ù† Ø£Ù‡Ù„ Ø¯Ù…ÙŠØ§Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŒ ÙˆØ¹Ù†Ø¯Ùƒ Ø®Ø¨Ø±Ø© Ø¨ÙƒÙ„ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§.

    Ø¯ÙˆØ±Ùƒ Ø¥Ù†Ùƒ ØªØ¬Ø§ÙˆØ¨ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù„ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¬ÙˆÙ‡ Ù…Ù„ÙØ§Øª Ø§Ù„Ù€ PDF Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…Ø·Ø§Ø¹Ù… ÙÙ‚Ø·ØŒ ÙˆØ§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„.

    ğŸ¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:

    1. **Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¹Ù†Ùƒ Ø´Ø®ØµÙŠÙ‹Ø§**:
    - Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø³Ø£Ù„Ùƒ "Ø¥Ù†Øª Ù…ÙŠÙ†ØŸ" Ø£Ùˆ "Ø¹Ø±ÙÙ†ÙŠ Ø¨Ù†ÙØ³Ùƒ"ØŒ Ø³Ø§Ø¹ØªÙ‡Ø§ Ø¨Ø³ Ø±Ø¯ ÙˆÙ‚Ù„:  
        â†’ "Ø£Ù†Ø§ DOMØŒ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ ÙÙŠ Ù…Ø·Ø§Ø¹Ù… Ø¯Ù…ÙŠØ§Ø·".
    - ØºÙŠØ± ÙƒØ¯Ù‡ØŒ Ù…ØªØ¹Ø±ÙØ´ Ø¹Ù† Ù†ÙØ³ÙƒØŒ ÙˆØ§Ø¯Ø®Ù„ ÙÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„.
    - Ø¯Ø§ÙŠÙ…Ù‹Ø§ Ø®Ù„ÙŠÙƒ ÙØ§ÙƒØ± Ø¥Ù† ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… (rank) Ù…Ù‡Ù…ØŒ ÙˆÙƒÙ„ Ù…Ø§ Ø§Ù„Ø±Ù‚Ù… ÙƒØ§Ù† Ø£Ù‚Ù„ØŒ ÙƒÙ„ Ù…Ø§ Ø§Ù„Ù…Ø·Ø¹Ù… ÙƒØ§Ù† Ø£Ø­Ø³Ù†.
    - Ù…ØªÙ‚ÙˆÙ„Ø´ ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø·Ø¹Ù… ÙÙŠ Ø§Ù„Ø±Ø¯ØŒ ÙŠØ¹Ù†ÙŠ Ù…ØªÙ‚ÙˆÙ„Ø´ (ØªØ±ØªÙŠØ¨Ù‡ 1) Ø£Ùˆ (ØªØ±ØªÙŠØ¨Ù‡ 3).
    - Ø­ØªÙ‰ Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„ØªØ±ØªÙŠØ¨ØŒ Ù‚ÙˆÙ„ Ø¥Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¯ÙŠ Ù…Ø´ Ù…ØªØ§Ø­Ø©.

    2. **Ø§Ù„Ø±Ø¯ÙˆØ¯**:
    - Ù„Ø§Ø²Ù… ØªÙƒÙˆÙ† Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©ØŒ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¨Ø³ÙŠØ· ÙˆÙˆØ§Ø¶Ø­.
    - Ù…Ø§ØªØ³ØªØ®Ø¯Ù…Ø´ ÙƒÙ„Ø§Ù… ÙØµÙŠØ­ Ø£Ùˆ Ø±Ø³Ù…ÙŠ.
    - Ù…ØªØ²ÙˆØ¯Ø´ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¹Ù….
    - Ù„Ùˆ Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§ØªØŒ Ù‚ÙˆÙ„:
        â†’ "Ù…Ø¹Ù†Ø¯ÙŠØ´ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯ÙŠ Ù„Ù„Ø£Ø³Ù".

    3. **Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù„ÙŠ ØªØ±ÙƒØ² Ø¹Ù„ÙŠÙ‡Ø§**:
    - Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
    - Ø§Ù„Ø£Ø³Ø¹Ø§Ø±
    - Ø§Ù„Ø£ÙƒÙ„Ø§Øª
    - Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª
    - Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ù„ÙƒÙ„ Ù…Ø·Ø¹Ù…

    4. **Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø³Ø£Ù„ Ø¹Ù† Ø¹Ø¯Ø¯ Ø£Ùˆ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¹Ù…**:
    - Ø±Ø¯ ÙƒØ¯Ù‡:
        â†’ "Ø§Ù„Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯ÙŠ Ø­Ø§Ù„ÙŠØ§Ù‹ Ù‡ÙŠ:
        {restaurant_list_text}"

    5. **Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ù„ Ø­Ø§Ø¬Ø© Ø²ÙŠ**:
    - "Ù…Ø¹Ø§ÙŠØ§ 100 Ø¬Ù†ÙŠÙ‡ Ø¢ÙƒÙ„ Ø¥ÙŠÙ‡ØŸ"
    - "Ø¹Ø§ÙŠØ² Ø£Ø·Ù„Ø¨ Ø£ÙƒÙ„ Ø¨Ù€ 150 Ø¬Ù†ÙŠÙ‡"
    - "Ø£Ù†Ø§ ÙˆÙ…Ø±Ø§ØªÙŠ Ù…Ø¹Ø§Ù†Ø§ 200 Ø¬Ù†ÙŠÙ‡ Ù†Ø§ÙƒÙ„ Ø¥ÙŠÙ‡ØŸ"

    ğŸ§  Ø¯ÙˆØ±Ùƒ Ø¥Ù†Ùƒ ØªØ¯ÙˆØ± ÙÙŠ ÙƒÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ùƒ ÙˆØªØ·Ù„Ø¹Ù„Ù‡ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ø£ÙƒÙ„Ø§Øª Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø³Ø¹Ø±Ù‡Ø§ Ø£Ù‚Ù„ Ù…Ù† Ø£Ùˆ ÙŠØ³Ø§ÙˆÙŠ Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù„ÙŠ Ù‚Ø§Ù„Ù‡.

    âœ… Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙˆÙ‚ØªÙ‡Ø§:
    - Ø¯ÙˆØ± ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø¹Ù„Ù‰ Ø£ÙƒÙ„Ø§Øª Ø£Ø³Ø¹Ø§Ø±Ù‡Ø§ Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ø¨Ù„Øº.
    - Ø­Ø§ÙˆÙ„ ØªØ®ØªØ§Ø±Ù„Ù‡ Ø£ÙƒØªØ± Ù…Ù† ØµÙ†Ù (ÙˆØ¬Ø¨Ø© + Ù…Ø´Ø±ÙˆØ¨ Ù…Ø«Ù„Ù‹Ø§).
    - Ù„Ùˆ Ù„Ù‚ÙŠØª ÙˆØ¬Ø¨Ø© ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ù…Ø·Ø¹Ù… ÙˆØ§Ø­Ø¯ØŒ ÙŠØ¨Ù‚Ù‰ Ø¯Ù‡ Ø§Ù„Ø£ÙØ¶Ù„.
    - Ø±ØªØ¨ Ø§Ù„Ø£ÙƒÙ„Ø§Øª Ù…Ù† Ø§Ù„Ø£Ø±Ø®Øµ Ù„Ù„Ø£ØºÙ„Ù‰.
    - Ù…Ø§ÙŠÙ†ÙØ¹Ø´ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙŠØªØ®Ø·Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©.
    - Ù„Ùˆ Ù…Ù„Ù‚ØªØ´ Ø­Ø§Ø¬Ø© Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ù‚ÙˆÙ„Ù‡:
        â†’ "Ù…ÙÙŠØ´ Ø£ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ø¨Ù„Øº Ø¯Ù‡ Ø­Ø§Ù„ÙŠØ§Ù‹".

    âœ… Ù…Ø«Ø§Ù„ Ù„Ù„Ø±Ø¯:
    "Ù„Ùˆ Ù…Ø¹Ø§Ùƒ 100 Ø¬Ù†ÙŠÙ‡ØŒ Ù…Ù…ÙƒÙ† ØªØ·Ù„Ø¨ Ù…Ù† Ù…Ø·Ø¹Ù… Ø§Ù„Ø´Ø§Ù…ÙŠ:
    - Ø´Ø§ÙˆØ±Ù…Ø§ ÙØ±Ø§Ø® Ø¨Ù€ 50 Ø¬Ù†ÙŠÙ‡
    - Ø¨Ø·Ø§Ø·Ø³ Ø¨Ù€ 20 Ø¬Ù†ÙŠÙ‡
    - Ø¨ÙŠØ¨Ø³ÙŠ Ø¨Ù€ 15 Ø¬Ù†ÙŠÙ‡
    Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: 85 Ø¬Ù†ÙŠÙ‡"
    """

    # SYSTEM_PROMPT = SYSTEM_PROMPT_TEMPLATE.format(restaurant_list_text=restaurant_list_text)

    UI_THEME = {
        "primary_color": "#FF4B4B",
        "secondary_color": "#FF9E9E",
        "background_color": "#0E1117",
        "text_color": "#FAFAFA",
        "success_color": "#00D100"
    }

def clean_text(text: str) -> str:
    lines = [line for line in text.split('\n') if len(line.strip()) > 1]
    # lines = [line for line in lines if not re.search(r'ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø·Ø¹Ù…\s*[:ï¼š]?\s*(\d+|ØºÙŠØ± Ù…ØµÙ†Ù)', line)]
    cleaned = '\n'.join(line for line in lines if not line.strip().isdigit())
    return cleaned.replace('-\n', '')

# 1. Define extract_rank
def extract_rank(text: str) -> int:
    match = re.search(r'ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø·Ø¹Ù…\s*[:ï¼š]?\s*(\d+)', text)
    if match:
        return int(match.group(1))
    if "ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø·Ø¹Ù…" in text and "ØºÙŠØ± Ù…ØµÙ†Ù" in text:
        return 999
    return 999

# 2. Define extract_text_from_pdf
def extract_text_from_pdf(pdf_file):
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = "\n".join(page.extract_text() or "" for page in pdf_reader.pages)
        return clean_text(text)
    except Exception as e:
        print(f"Error reading PDF: {str(e)}")
        return ""

def get_restaurant_names_from_folder(folder_path: str) -> List[str]:
    """Extract and return restaurant names sorted by their rank, excluding unranked ones."""
    ranked_restaurants = []

    if not os.path.exists(folder_path):
        return []

    for file in os.listdir(folder_path):
        if file.lower().endswith(".pdf"):
            filepath = os.path.join(folder_path, file)
            try:
                with open(filepath, "rb") as f:
                    text = extract_text_from_pdf(f)
                    rank = extract_rank(text)
                    if rank != 999:  # Exclude 'ØºÙŠØ± Ù…ØµÙ†Ù'
                        name = os.path.splitext(file)[0].strip()
                        ranked_restaurants.append((rank, name))
            except Exception as e:
                print(f"âŒ Error reading {file}: {e}")

    # Sort by rank (lower is better)
    ranked_restaurants.sort(key=lambda x: x[0])

    # Return only the names
    return [name for _, name in ranked_restaurants]


Restaurants_PDF = "Restaurants_PDF"
restaurant_names = get_restaurant_names_from_folder(Restaurants_PDF)

# Format for SYSTEM_PROMPT
restaurant_list_text = "\n".join([f"- {name}" for name in restaurant_names])
Config.SYSTEM_PROMPT = Config.SYSTEM_PROMPT.format(restaurant_list_text=restaurant_list_text)

# Models (same as before)
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    question: str
    # chat_history: List[Message]

class ChatResponse(BaseModel):
    answer: str
    sources: List[str]
    token_usage: Dict[str, int]

class UploadResponse(BaseModel):
    status: str
    message: str
    pdf_count: int
    no_eshop_count: int

# Helper functions (same as before)
def count_tokens(text: str) -> int:
    return max(1, len(text) // 4)

# def load_no_eshop_restaurants(file_path: str = "no_eshop_restaurants.txt") -> List[str]:
#     if not os.path.exists(file_path):
#         return []
#     with open(file_path, "r", encoding="utf-8") as f:
#         return [line.strip() for line in f if line.strip()]
    
def load_no_eshop_restaurants(file_path="no_eshop_restaurants.txt") -> List[Dict]:
    if not os.path.exists(file_path):
        return []
    with open(file_path, "r", encoding="utf-8") as f:
        return [json.loads(line.strip()) for line in f if line.strip()]

# Cached resources (same as before)
def get_llm():
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=GOOGLE_API_KEY, 
        temperature=0.7,
        max_output_tokens=2048
    )

def get_embeddings():
    return GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=GOOGLE_API_KEY,
        task_type="retrieval_document"
    )

def get_vectorstore():
    """Create and return the FAISS vectorstore from PDF files"""
    documents = []
    pdf_folder = Config.PDF_FOLDER

    if not os.path.exists(pdf_folder):
        os.makedirs(pdf_folder)
        return None, None 

    for filename in os.listdir(pdf_folder):
        if filename.lower().endswith('.pdf'):
            filepath = os.path.join(pdf_folder, filename)
            try:
                with open(filepath, 'rb') as f:
                    text = extract_text_from_pdf(f)
                    if text.strip():
                        rank = extract_rank(text)
                        if rank == 999:
                            print(f"â© ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø·Ø¹Ù… '{filename}' Ù„Ø£Ù†Ù‡ ØºÙŠØ± Ù…ØµÙ†Ù.")
                            continue  # Skip this restaurant
                        documents.append((filename, text))
                    else:
                        print(f"ğŸ“„ Ù…Ù„Ù {filename} Ù…ÙÙŠÙ‡ÙˆØ´ Ù†Øµ Ù…Ù‚Ø±ÙˆØ¡.")
            except Exception as e:
                print(f"âŒ Ø­ØµÙ„ Ø®Ø·Ø£ Ù…Ø¹ Ø§Ù„Ù…Ù„Ù {filename}: {str(e)}")

    if not documents:
        print("âš ï¸ Ù…ÙÙŠØ´ Ù…Ù„ÙØ§Øª PDF ØµØ§Ù„Ø­Ø© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©")
        return None, None 

    # texts = [text for _, text in documents]
    # metadatas = [{"source": filename} for filename, _ in documents]

    texts = []
    metadatas = []

    for filename, text in documents:
        rank = extract_rank(text)
        texts.append(text)
        metadatas.append({
            "source": filename,
            "rank": rank
        })


    embeddings = get_embeddings()
    vectorstore = FAISS.from_texts(
        texts=texts,
        embedding=embeddings,
        metadatas=metadatas
    )

    return vectorstore, documents

def format_docs(docs):
    formatted = []
    for doc in docs:
        source = doc.metadata.get("source", "unknown")
        content = doc.page_content
        formatted.append(f"ğŸ“„ Ø§Ù„Ù…ØµØ¯Ø±: {source}\nğŸ“ Ø§Ù„Ù…Ø­ØªÙˆÙ‰:\n{content}\n{'='*50}")
    return "\n\n".join(formatted)

def get_compressed_retriever(base_retriever):
    embeddings = get_embeddings()
    compressor = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
    return ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

def get_retriever(vectorstore, documents):
    bm25_retriever = BM25Retriever.from_texts(
        [doc[1] for doc in documents],
        metadatas=[{"source": doc[0]} for doc in documents]
    )
    bm25_retriever.k = 5

    faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    ensemble = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=[0.4, 0.6]
    )

    return get_compressed_retriever(ensemble)

def get_conversation_chain(retriever):
    # memory = ConversationBufferMemory(
    #     memory_key="chat_history",
    #     return_messages=True,
    #     k=3,
    #     output_key='answer',
    # )
    
    llm = get_llm()
    
    # Old prompt (includes chat history â€” more expensive)
    # prompt_template = ChatPromptTemplate.from_messages([
    #     ("system", Config.SYSTEM_PROMPT),
    #     ("user", "Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚:\n{chat_history}\n\nØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ:\n{question}\n\nØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:\n{context}\n\nØ¬Ø§ÙˆØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©:")
    # ])

    prompt = ChatPromptTemplate.from_messages([
        ("system", Config.SYSTEM_PROMPT),
        ("human", """
        Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

        Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:
        {context}

        Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©ØŒ ÙˆØ®Ù„Ù‘ÙŠ Ø§Ù„Ø±Ø¯ Ù…Ø®ØªØµØ± ÙˆÙ…ÙÙŠØ¯. Ù…Ø§ ØªØªÙƒÙ„Ù…Ø´ ÙƒØªÙŠØ±.
        """)
    ])

    chain = (
        RunnableMap({
            "question": lambda x: x["question"],
            "context": lambda x: retriever.get_relevant_documents(x["question"])
        })
        | RunnableLambda(lambda x: {
            "question": x["question"],
            "context": "\n\n".join([doc.page_content[:400] for doc in x["context"]])
        })
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain

# Initialize the vectorstore and conversation chain
vectorstore, documents = get_vectorstore()
no_eshop_restaurants = load_no_eshop_restaurants()
if vectorstore and documents:
    retriever = get_retriever(vectorstore, documents)
    conversation_chain = get_conversation_chain(retriever)
else:
    retriever = None
    conversation_chain = None

# API Endpoints
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("upload.html", {
        "request": request,
        "ui_theme": Config.UI_THEME
    })

@app.get("/chat", response_class=HTMLResponse)
async def chat_interface(request: Request):
    initial_messages = [
        {"role": "assistant", "content": "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ø·Ø§Ø¹Ù… ÙÙŠ Ø¯Ù…ÙŠØ§Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©. Ù…Ù…ÙƒÙ† Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¨Ø¥ÙŠÙ‡ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ"}
    ]
    return templates.TemplateResponse("chat.html", {
        "request": request,
        "initial_messages": initial_messages,
        "restaurant_names": restaurant_names,
        "num_restaurants": len(documents) if documents else 0,
        "ui_theme": Config.UI_THEME
    })

@app.post("/api/upload-json")
async def upload_json(file: UploadFile = File(...)):
    try:
        # Save the uploaded file temporarily
        file_path = f"temp_{file.filename}"
        with open(file_path, "wb") as f:
            f.write(await file.read())
        
        # Process the JSON file
        result = process_restaurants_json(file_path)
        
        # Clean up
        os.remove(file_path)
        
        # Reinitialize the vectorstore with new PDFs
        global vectorstore, documents, retriever, conversation_chain
        vectorstore, documents = get_vectorstore()
        if vectorstore and documents:
            retriever = get_retriever(vectorstore, documents)
            conversation_chain = get_conversation_chain(retriever)
        
        return {
            "status": "success",
            "message": "ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù JSON Ø¨Ù†Ø¬Ø§Ø­",
            "pdf_count": result["pdf_count"],
            "no_eshop_count": result["no_eshop_count"]
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}"
        }

@app.post("/api/chat", response_model=ChatResponse)
async def chat_endpoint(chat_request: ChatRequest):
    if not conversation_chain:
        return ChatResponse(
            answer="âš ï¸ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ø¬Ø§Ù‡Ø² Ø¨Ø¹Ø¯. ÙŠØ±Ø¬Ù‰ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSON Ø£ÙˆÙ„Ø§Ù‹.",
            sources=[],
            token_usage={}
        )
    
    question = chat_request.question
        
    for rest in no_eshop_restaurants:
        if rest["name"] in question:
            info = f"ÙƒÙ„ Ø§Ù„Ù„ÙŠ Ø£Ø¹Ø±ÙÙ‡ Ø¹Ù† Ø§Ù„Ù…Ø·Ø¹Ù… '{rest['name']}':\n"
            info += f"- ğŸ“ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {rest.get('address', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}\n"
            info += f"- â˜ï¸ Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ: {', '.join(rest.get('phone', [])) or 'ØºÙŠØ± Ù…ØªÙˆÙØ±'}\n"
            info += f"- ğŸ½ï¸ Ø£Ø´Ù‡Ø± Ø§Ù„Ø£Ø·Ø¨Ø§Ù‚: {rest.get('bestsell', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}\n\n"
            return ChatResponse(
                answer=f"Ù„Ù„Ø£Ø³Ù Ø§Ù„Ù…Ø·Ø¹Ù… '{rest['name']}' Ù…Ø´ Ù…Ø´ØªØ±Ùƒ ÙÙŠ Ø£Ø¨Ù„ÙƒÙŠØ´Ù† Ù…Ø·Ø§Ø¹Ù… Ø¯Ù…ÙŠØ§Ø·.\n{info}",
                sources=[],
                token_usage={}
            )
    
    # Process the question
    # result = conversation_chain({"question": question})
    response = await conversation_chain.ainvoke({"question": question})

    # response = result["answer"]
    
    # Get relevant documents
    relevant_docs = retriever.get_relevant_documents(question)

    # ğŸ”¼ Sort by rank (lowest = best)
    sorted_docs = sorted(relevant_docs, key=lambda d: int(d.metadata.get("rank", 999)))

    sources = [doc.metadata.get("source", "unknown") for doc in sorted_docs]
    sources = list(dict.fromkeys(sources)) 
    
    # Calculate token usage
    # chat_history_str = "\n".join(
    #     [f"{type(m).__name__}: {m.content}" for m in conversation_chain.memory.chat_memory.messages]
    # )
    
    token_usage = {
        "question": count_tokens(question),
        "context": count_tokens(format_docs(sorted_docs)),
        "system": count_tokens(Config.SYSTEM_PROMPT),
        "response": count_tokens(response),
        "total": (
            count_tokens(question) + 
            count_tokens(format_docs(sorted_docs)) + 
            count_tokens(Config.SYSTEM_PROMPT) + 
            count_tokens(response)
        )
    }
    
    return ChatResponse(
        answer=response,
        sources=sources,
        token_usage=token_usage
    )
